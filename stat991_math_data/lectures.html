<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>STAT 991-302: Mathematics of High-Dimensional Data </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">STAT 991-302</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="course_info.html">Course&nbsp;Info</a></div>
<div class="menu-item"><a href="syllabus.html">Syllabus</a></div>
<div class="menu-item"><a href="lectures.html">Lectures</a></div>
<div class="menu-item"><a href="homework.html">Homework</a></div>
<div class="menu-item"><a href="project.html">Project</a></div>
<div class="menu-item"><a href="reference.html">Reference</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>STAT 991-302: Mathematics of High-Dimensional Data </h1>
<div id="subtitle"><a href="https://yuxinchen2020.github.io/">Yuxin Chen</a>, Wharton Statistics and Data Science, Spring 2022</div>
</div>
<h2>Lecture notes</h2>
<p>These lecture notes will be updated frequently; see also <a href="https://yuxinchen2020.github.io/ele520_math_data/index.html">lectures from previous offerings</a>.</p>
<ul>
<li><p><a href="lectures/spectral_method.pdf">Spectral methods</a></p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/2012.08496">Spectral methods for data science: A statistical perspective</a></p>
</li></ul>
</li>
<li><p><a href="lectures/matrix_concentration.pdf">Matrix concentration inequalities</a></p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1501.01571">An introduction to matrix concentration inequalities</a></p>
</li></ul>
</li>
<li><p><a href="lectures/large_scale_eigenvalue_problems.pdf">Large-scale eigenvalue problems (power method and Lanczos algorithm)</a></p>
<ul>
<li><p>Reading: <a href="http://web.mit.edu/ehliu/Public/sclark/Golub%20G.H.,%20Van%20Loan%20C.F.-%20Matrix%20Computations.pdf">Matrix computations, Chapter 9</a></p>
</li></ul>
</li>
<li><p><a href="lectures/tensor_decomposition.pdf">Tensor decomposition</a></p>
<ul>
<li><p>Reading: <a href="http://jmlr.org/papers/v15/anandkumar14b.html">Tensor decompositions for learning latent variable models</a></p>
</li>
<li><p>Reading: <a href="https://arxiv.org/abs/2012.08496">Spectral methods for data science: A statistical perspective (Section 3.9)</a></p>
</li></ul>
</li>
<li><p><a href="lectures/randomized_linear_algebra.pdf">Randomized linear algebra</a></p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1608.04481">Lecture notes on randomized linear algebra</a></p>
</li></ul>
</li>
<li><p><a href="lectures/compressed_sensing.pdf">Compressed sensing and sparse recovery</a></p>
<ul>
<li><p>Reading: <a href="https://statweb.stanford.edu/~candes/papers/ICM2014.pdf">Mathematics of sparsity (and a few other things)</a></p>
</li></ul>
</li>
<li><p><a href="lectures/phase_transition.pdf">Phase transition and convex geometry</a>  </p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1303.6672">Living on the edge: Phase transitions in convex programs with random data</a></p>
</li></ul>
</li>
<li><p><a href="lectures/matrix_recovery.pdf">Low-rank matrix recovery</a></p>
<ul>
<li><p>Reading: <a href="https://statweb.stanford.edu/~candes/papers/ICM2014.pdf">Mathematics of sparsity (and a few other things)</a></p>
</li></ul>
</li>
<li><p><a href="lectures/robust_PCA.pdf">Robust principal component analysis</a></p>
</li>
<li><p>Nonconvex matrix factorization: <a href="lectures/nonconvex_part_i.pdf">Part 1</a>, <a href="lectures/nonconvex_part_ii.pdf">Part 2</a>, <a href="lectures/nonconvex_part_iii.pdf">Part 3</a></p>
<ul>
<li><p>Reading: <a href="https://arxiv.org/abs/1809.09573">Nonconvex optimization meets low-rank matrix factorization: An overview</a></p>
</li></ul>
</li>
<li><p>Reinforcement learning: <a href="lectures/RL_short_part1.pdf">Part 1</a>, <a href="lectures/RL_short_part2.pdf">Part 2</a></p>
</li>
</ul>
<h2>Supplementary readings (not to be covered in class)</h2>
<ul>
<li><p><a href="lectures/sparse_representation.pdf">Sparse representation</a></p>
<ul>
<li><p>Reading: <a href="https://ieeexplore.ieee.org/document/959265">Uncertainty principles and ideal atomic decomposition</a>







</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2022-04-26 23:46:11 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
