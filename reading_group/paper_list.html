<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Mathematical Data Science Reading Group </title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Reading group</div>
<div class="menu-item"><a href="index.html">Schedule</a></div>
<div class="menu-item"><a href="paper_list.html">Reading&nbsp;list</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Mathematical Data Science Reading Group </h1>
<div id="subtitle"><a href="http://www.princeton.edu/~yc5/">Yuxin Chen</a>, Princeton University</div>
</div>
<div class="infoblock">
<div class="blockcontent">
<p>Here are a few recent or classical papers that interest me.  Please select one from below for presentation. </p>
</div></div>
<h2>Over-parametrization and implicit bias</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/1710.10345">“The Implicit Bias of Gradient Descent on Separable Data,”</a> Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro, 2017
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1803.07300">“Risk and parameter convergence of logistic regression,”</a> Ziwei Ji, Matus Telgarsky, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.05040">“How do infinite width bounded norm networks look in function space?”</a> Pedro Savarese, Itay Evron, Daniel Soudry, Nathan Srebro, 2019 
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.04674">“Towards moderate overparameterization: global convergence guarantees for training shallow neural networks,”</a> Samet Oymak, Mahdi Soltanolkotabi, 2019</p>
</li>
</ul>
<h2>Neural nets theory</h2>
<ul>
<li><p><a href="http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf">“Universal approximation bounds for superpositions of a sigmoidal function,”</a> Andrew Barron, 1993
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1806.07572">“Neural Tangent Kernel: Convergence and Generalization in Neural Networks,”</a> A. Jacot, F. Gabriel, C. Hongler, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.01028">“Can SGD Learn Recurrent Neural Networks with Provable Generalization?”</a> Z. Allen-Zhu, Y. Li, 2019
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1805.09545.pdf">“On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,”</a> Chizat, Bach, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.04674">“Towards moderate overparameterization: global convergence guarantees for training shallow neural networks,”</a> S. Oymak, M. Soltanolkotabi, 2019
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1811.04918.pdf">“Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers,”</a> Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1804.06561">“A Mean Field View of the Landscape of Two-Layers Neural Networks,”</a> Song Mei, Andrea Montanari, Phan-Minh Nguyen, PNAS, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1811.04918">“Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers,”</a> Z. Allen-Zhu, Y. Li, Y. Liang, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1808.01204">“Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,”</a> Y. Li, Y. Liang, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1810.05369.pdf">“On the Margin Theory of Feedforward Neural Networks,”</a> C. Wei, J. Lee, Q. Liu, and T. Ma, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1802.07301">“On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition,”</a> M. Mondelli, A. Montanari, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1809.08587">“Exponential Convergence Time of Gradient Descent for One-Dimensional Deep Linear Neural Networks,”</a> O. Shamir, 2018 
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1812.11118">“Reconciling modern machine learning and the bias-variance trade-off,”</a> M. Belkin, D. Hsu, S. Ma, and S. Mandal, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1902.01996.pdf">“Are all layers created equal?”</a> Chiyuan Zhang, Samy Bengio, Yoram Singer, 2019
</p>
</li>
<li><p><a href="http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf">“Do ImageNet Classifiers Generalize to ImageNet?”</a> Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, 2019 
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1810.02030.pdf">“Robust Estimation and Generative Adversarial Nets”</a> Chao Gao, Jiyi Liu, Yuan Yao and Weizhi Zhu, 2018</p>
</li>
</ul>
<h2>Large-scale optimization</h2>
<ul>
<li><p><a href="http://web.mit.edu/jnt/www/Papers/J014-86-asyn-grad.pdf">“Distributed asynchronous deterministic and stochastic gradient optimization algorithms,”</a> J Tsitsiklis, D Bertsekas, M Athans, 1986
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1702.08704">“Optimal algorithms for smooth and strongly convex distributed optimization in networks,”</a> K. Scaman, F. Bach, S. Bubeck, Y.T. Lee, L. Massoulié, 2017
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.00618">“Minmax Optimization: Stable Limit Points of Gradient Descent Ascent are Locally Optimal,”</a> C. Jin, P. Netrapalli and M. Jordan, 2019
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.00247">“Sharp Analysis for Nonconvex SGD Escaping from Saddle Points,”</a> C. Fang, Z. Lin, T. Zhang, 2019
</p>
</li>
<li><p><a href="https://openreview.net/forum?id=ryQu7f-RZ&amp;utm_campaign=Revue%20newsletter&amp;utm_medium=Newsletter&amp;utm_source=piqcy">“On the convergence of Adam and beyond,"</a> Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar, ICLR 2018.  
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1901.09149">“Escaping Saddle Points with Adaptive Gradient Methods,”</a>  M. Staib, S. Reddi, S. Kale, S. Kumar, S. Sra, 2019
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1603.05953">“Katyusha: the first direct acceleration of stochastic gradient methods,”</a> Z. Allen-Zhu, STOC, 2017
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1804.07795">“Stochastic subgradient method converges on tame functions,”</a> D. Davis, D. Drusvyatskiy, S. Kakade, and J. Lee, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1809.08530">“Provably Correct Automatic Subdifferentiation for Qualified Programs,”</a> S. Kakade, J. Lee, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1805.11604.pdf">“How Does Batch Normalization Help Optimization?”</a> Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1807.01695">“SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator,”</a> Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1605.07112">“Harnessing Smoothness to Accelerate Distributed Optimization,”</a> G. Qu, N. Li, 2016</p>
</li>
</ul>
<h2>Reinforcement learning and control</h2>
<ul>
<li><p><a href="http://www.mit.edu/~jnt/Papers/J094-03-kon-actors.pdf">“On actor-critic algorithms,”</a> Vijay R Konda, John N Tsitsiklis, 2000
</p>
</li>
<li><p><a href="https://link.springer.com/article/10.1007/BF00993306">“Asynchronous stochastic approximation and Q-learning,”</a> John N Tsitsiklis, 1994
</p>
</li>
<li><p><a href="http://proceedings.mlr.press/v80/fazel18a.html">“Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator,”</a> M. Fazel, R. Ge, S. Kakade, M. Mesbahi, ICML 2018
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1812.03565.pdf">“The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint,”</a> S. Tu, B. Recht, 2018
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1902.00923">“Finite-Time Error Bounds For Linear Stochastic Approximation and TD Learning,”</a> R. Srikant, L. Ying, 2019
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1812.00885">“AsyncQVI: Asynchronous-Parallel Q-Value Iteration for Reinforcement Learning with Near-Optimal Sample Complexity,”</a> Y. Zeng, F. Feng, W. Yin, 2018
</p>
</li>
<li><p><a href="https://dl.acm.org/citation.cfm?id=1496876">&ldquo;Column subset selection, matrix factorization, and eigenvalue optimization,&rdquo;</a> J. Tropp, SODA, 2009.</p>
</li>
</ul>
<h2>Statistics, probability, &hellip;</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/1404.1392">“A short survey of Stein's method,”</a> Sourav Chatterjee, 2014 
</p>
</li>
<li><p><a href="https://arxiv.org/abs/1201.6002">“Matrix concentration inequalities via the method of exchangeable pairs,”</a> Lester Mackey, Michael I. Jordan, Richard Y. Chen, Brendan Farrell, Joel A. Tropp, 2012
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1408.3470.pdf">“Efron–Stein Inequalities for Random Matrices,”</a> Daniel Paulin, Lester Mackey and Joel A. Tropp, 2014
</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1405.1102.pdf">“Convex recovery of a structured signal from independent random linear measurements,”</a> Joel A. Tropp, 2014
</p>
</li>
<li><p><a href="http://emis.ams.org/journals/EJP-ECP/article/download/2210/2210-11637-1-PB.pdf">“Concentration inequalities for order statistics,"</a> S. Boucheron and M. Thomas, Electronic Communications in Probability, 2012.</p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-06-17 22:48:34 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
