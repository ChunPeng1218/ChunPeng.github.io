# jemdoc: menu{menu.jemdoc}{contact.html}
= Mathematical Data Science Reading Group 
[http://www.princeton.edu/~yc5/ Yuxin Chen], Princeton University


~~~
Here are a few recent or classical papers that interest me.  Please select one from below for presentation. 
~~~

== Over-parametrization and implicit bias

- [https://arxiv.org/abs/1710.10345 “The Implicit Bias of Gradient Descent on Separable Data,”] Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, Nathan Srebro, 2017
#
- [https://arxiv.org/abs/1803.07300 “Risk and parameter convergence of logistic regression,”] Ziwei Ji, Matus Telgarsky, 2018
#
- [https://arxiv.org/abs/1902.05040 “How do infinite width bounded norm networks look in function space?”] Pedro Savarese, Itay Evron, Daniel Soudry, Nathan Srebro, 2019 
#
- [https://arxiv.org/abs/1902.04674 “Towards moderate overparameterization: global convergence guarantees for training shallow neural networks,”] Samet Oymak, Mahdi Soltanolkotabi, 2019
 
== Neural nets theory
#
- [http://www.stat.yale.edu/~arb4/publications_files/UniversalApproximationBoundsForSuperpositionsOfASigmoidalFunction.pdf “Universal approximation bounds for superpositions of a sigmoidal function,”] Andrew Barron, 1993
#
- [https://arxiv.org/abs/1806.07572 “Neural Tangent Kernel: Convergence and Generalization in Neural Networks,”] A. Jacot, F. Gabriel, C. Hongler, 2018
#
- [https://arxiv.org/abs/1902.01028 “Can SGD Learn Recurrent Neural Networks with Provable Generalization?”] Z. Allen-Zhu, Y. Li, 2019
#
- [https://arxiv.org/pdf/1805.09545.pdf “On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport,”] Chizat, Bach, 2018
#
- [https://arxiv.org/abs/1902.04674 “Towards moderate overparameterization: global convergence guarantees for training shallow neural networks,”] S. Oymak, M. Soltanolkotabi, 2019
#
- [https://arxiv.org/pdf/1811.04918.pdf “Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers,”] Zeyuan Allen-Zhu, Yuanzhi Li, Yingyu Liang, 2018
#
- [https://arxiv.org/abs/1804.06561 “A Mean Field View of the Landscape of Two-Layers Neural Networks,”] Song Mei, Andrea Montanari, Phan-Minh Nguyen, PNAS, 2018
#
- [https://arxiv.org/abs/1811.04918 “Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers,”] Z. Allen-Zhu, Y. Li, Y. Liang, 2018
#
- [https://arxiv.org/abs/1808.01204 “Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data,”] Y. Li, Y. Liang, 2018
#
- [https://arxiv.org/pdf/1810.05369.pdf “On the Margin Theory of Feedforward Neural Networks,”] C. Wei, J. Lee, Q. Liu, and T. Ma, 2018
#
- [https://arxiv.org/abs/1802.07301 “On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition,”] M. Mondelli, A. Montanari, 2018
#
- [https://arxiv.org/abs/1809.08587 “Exponential Convergence Time of Gradient Descent for One-Dimensional Deep Linear Neural Networks,”] O. Shamir, 2018 
#
- [https://arxiv.org/abs/1812.11118 “Reconciling modern machine learning and the bias-variance trade-off,”] M. Belkin, D. Hsu, S. Ma, and S. Mandal, 2018
#
- [https://arxiv.org/pdf/1902.01996.pdf “Are all layers created equal?”] Chiyuan Zhang, Samy Bengio, Yoram Singer, 2019
#
- [http://people.csail.mit.edu/ludwigs/papers/imagenet.pdf “Do ImageNet Classifiers Generalize to ImageNet?”] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, 2019 
#
- [https://arxiv.org/pdf/1810.02030.pdf “Robust Estimation and Generative Adversarial Nets”] Chao Gao, Jiyi Liu, Yuan Yao and Weizhi Zhu, 2018


== Large-scale optimization
#
- [http://web.mit.edu/jnt/www/Papers/J014-86-asyn-grad.pdf “Distributed asynchronous deterministic and stochastic gradient optimization algorithms,”] J Tsitsiklis, D Bertsekas, M Athans, 1986
#
- [https://arxiv.org/abs/1702.08704 “Optimal algorithms for smooth and strongly convex distributed optimization in networks,”] K. Scaman, F. Bach, S. Bubeck, Y.T. Lee, L. Massoulié, 2017
#
- [https://arxiv.org/abs/1902.00618 “Minmax Optimization: Stable Limit Points of Gradient Descent Ascent are Locally Optimal,”] C. Jin, P. Netrapalli and M. Jordan, 2019
#
- [https://arxiv.org/abs/1902.00247 “Sharp Analysis for Nonconvex SGD Escaping from Saddle Points,”] C. Fang, Z. Lin, T. Zhang, 2019
#
- [https://openreview.net/forum?id=ryQu7f-RZ&utm_campaign=Revue%20newsletter&utm_medium=Newsletter&utm_source=piqcy “On the convergence of Adam and beyond,"] Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar, ICLR 2018.  
#
- [https://arxiv.org/abs/1901.09149 “Escaping Saddle Points with Adaptive Gradient Methods,”]  M. Staib, S. Reddi, S. Kale, S. Kumar, S. Sra, 2019
#
- [https://arxiv.org/abs/1603.05953 “Katyusha: the first direct acceleration of stochastic gradient methods,”] Z. Allen-Zhu, STOC, 2017
#
- [https://arxiv.org/abs/1804.07795 “Stochastic subgradient method converges on tame functions,”] D. Davis, D. Drusvyatskiy, S. Kakade, and J. Lee, 2018
#
- [https://arxiv.org/abs/1809.08530 “Provably Correct Automatic Subdifferentiation for Qualified Programs,”] S. Kakade, J. Lee, 2018
#
- [https://arxiv.org/pdf/1805.11604.pdf “How Does Batch Normalization Help Optimization?”] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, Aleksander Madry, 2018
#
- [https://arxiv.org/abs/1807.01695 “SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator,”] Cong Fang, Chris Junchi Li, Zhouchen Lin, Tong Zhang, 2018
#
- [https://arxiv.org/abs/1605.07112 “Harnessing Smoothness to Accelerate Distributed Optimization,”] G. Qu, N. Li, 2016

== Reinforcement learning and control

#
- [http://www.mit.edu/~jnt/Papers/J094-03-kon-actors.pdf “On actor-critic algorithms,”] Vijay R Konda, John N Tsitsiklis, 2000
#
- [https://link.springer.com/article/10.1007/BF00993306 “Asynchronous stochastic approximation and Q-learning,”] John N Tsitsiklis, 1994
#
- [http://proceedings.mlr.press/v80/fazel18a.html “Global Convergence of Policy Gradient Methods for the Linear Quadratic Regulator,”] M. Fazel, R. Ge, S. Kakade, M. Mesbahi, ICML 2018
#
- [https://arxiv.org/pdf/1812.03565.pdf “The Gap Between Model-Based and Model-Free Methods on the Linear Quadratic Regulator: An Asymptotic Viewpoint,”] S. Tu, B. Recht, 2018
#
- [https://arxiv.org/abs/1902.00923 “Finite-Time Error Bounds For Linear Stochastic Approximation and TD Learning,”] R. Srikant, L. Ying, 2019
#
- [https://arxiv.org/abs/1812.00885 “AsyncQVI: Asynchronous-Parallel Q-Value Iteration for Reinforcement Learning with Near-Optimal Sample Complexity,”] Y. Zeng, F. Feng, W. Yin, 2018
#
- [https://dl.acm.org/citation.cfm?id=1496876 "Column subset selection, matrix factorization, and eigenvalue optimization,"] J. Tropp, SODA, 2009.

#== Optimal transport


== Statistics, probability, ...
#
- [https://arxiv.org/abs/1404.1392 “A short survey of Stein's method,”] Sourav Chatterjee, 2014 
#
- [https://arxiv.org/abs/1201.6002 “Matrix concentration inequalities via the method of exchangeable pairs,”] Lester Mackey, Michael I. Jordan, Richard Y. Chen, Brendan Farrell, Joel A. Tropp, 2012
#
- [https://arxiv.org/pdf/1408.3470.pdf “Efron–Stein Inequalities for Random Matrices,”] Daniel Paulin, Lester Mackey and Joel A. Tropp, 2014
#
- [https://arxiv.org/pdf/1405.1102.pdf “Convex recovery of a structured signal from independent random linear measurements,”] Joel A. Tropp, 2014
#
- [http://emis.ams.org/journals/EJP-ECP/article/download/2210/2210-11637-1-PB.pdf “Concentration inequalities for order statistics,"] S. Boucheron and M. Thomas, Electronic Communications in Probability, 2012.






